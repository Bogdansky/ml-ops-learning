from pathlib import Path
import json
import argparse

import numpy as np
from sentence_transformers import SentenceTransformer


def load_embeddings(emb_path: Path) -> np.ndarray:
    if not emb_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {emb_path}")
    embeddings = np.load(emb_path)
    if embeddings.ndim != 2:
        raise ValueError(f"–û–∂–∏–¥–∞–ª–∞—Å—å –º–∞—Ç—Ä–∏—Ü–∞ 2D, –∞ –Ω–µ —Ñ–æ—Ä–º–∞ {embeddings.shape}")
    return embeddings


def load_meta(meta_path: Path) -> list[dict]:
    if not meta_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {meta_path}")
    data = json.loads(meta_path.read_text(encoding="utf-8"))
    if not isinstance(data, list):
        raise ValueError("–û–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ –≤ chunks_meta.json")
    return data


def build_model() -> SentenceTransformer:
    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    print(f"‚ñ∂ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
    model = SentenceTransformer(model_name)
    return model


def search(
    query: str,
    model: SentenceTransformer,
    embeddings: np.ndarray,
    meta: list[dict],
    top_k: int = 3,
):
    # –°—á–∏—Ç–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_vec = model.encode(
        [query],
        convert_to_numpy=True,
        normalize_embeddings=True,  # —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ embed_chunks.py
    )[0]

    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ = —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
    scores = embeddings @ query_vec  # shape: (num_chunks,)

    # –ë–µ—Ä—ë–º top_k –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    top_k = min(top_k, len(scores))
    top_indices = np.argsort(scores)[::-1][:top_k]

    results = []
    for rank, idx in enumerate(top_indices, start=1):
        score = float(scores[idx])
        chunk_info = meta[idx]
        results.append(
            {
                "rank": rank,
                "score": score,
                "chunk_id": chunk_info.get("chunk_id"),
                "text": chunk_info.get("text", ""),
            }
        )
    return results


def pretty_print_results(results: list[dict], max_chars: int = 400):
    if not results:
        print("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    print("\n========= –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê =========")
    for r in results:
        text = r["text"].replace("\n", " ")
        if len(text) > max_chars:
            text_preview = text[:max_chars].rstrip() + "..."
        else:
            text_preview = text

        print(f"\n#{r['rank']}  (score={r['score']:.4f}, chunk_id={r['chunk_id']})")
        print(text_preview)
    print("=====================================\n")


def main():
    parser = argparse.ArgumentParser(description="–ü–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º —Å –ø–æ–º–æ—â—å—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    parser.add_argument(
        "--embeddings",
        type=str,
        default="embeddings.npy",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ (.npy)",
    )
    parser.add_argument(
        "--meta",
        type=str,
        default="chunks_meta.json",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–æ —á–∞–Ω–∫–∞–º (.json)",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=3,
        help="–°–∫–æ–ª—å–∫–æ –±–ª–∏–∂–∞–π—à–∏—Ö —á–∞–Ω–∫–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å",
    )

    args = parser.parse_args()

    emb_path = Path(args.embeddings)
    meta_path = Path(args.meta)

    print("‚ñ∂ –ó–∞–≥—Ä—É–∂–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    embeddings = load_embeddings(emb_path)
    print(f"–§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")

    print("‚ñ∂ –ó–∞–≥—Ä—É–∂–∞—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞–Ω–∫–∞–º...")
    meta = load_meta(meta_path)
    print(f"–ß–∞–Ω–∫–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(meta)}")

    if embeddings.shape[0] != len(meta):
        raise ValueError(
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({embeddings.shape[0]}) "
            f"–Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —á–∞–Ω–∫–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ({len(meta)})"
        )

    model = build_model()

    print("\n–ì–æ—Ç–æ–≤–æ. –ú–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å.")
    print('–í–≤–µ–¥–∏ –∑–∞–ø—Ä–æ—Å –∏ –Ω–∞–∂–º–∏ Enter. –î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –∏–ª–∏ "q".\n')

    while True:
        try:
            query = input("üîé –ó–∞–ø—Ä–æ—Å: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n–í—ã—Ö–æ–¥.")
            break

        if not query or query.lower() in {"q", "quit", "exit"}:
            print("–í—ã—Ö–æ–¥.")
            break

        results = search(
            query=query,
            model=model,
            embeddings=embeddings,
            meta=meta,
            top_k=args.top_k,
        )
        pretty_print_results(results)


if __name__ == "__main__":
    main()
